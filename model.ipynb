{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c89a62fd-3db4-45f5-9703-22bc73d8e20f",
   "metadata": {},
   "source": [
    "`transformer - language translation`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80897f7-8556-480d-b1ed-0272203644fb",
   "metadata": {},
   "source": [
    "# model\n",
    "> code and explanation of the transformer model architecture.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1280/1*oh6wljc7WoW8G-0KNNBJww.jpeg\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad2c0df-057d-4875-8cde-856d0f486990",
   "metadata": {},
   "source": [
    "> * **transformer model** is a transduction model.\n",
    "> * **transduction model** is a type of neural network model that maps input sequence to output sequence of **variable length**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a490304-0c60-4bd0-bd28-48ae8ceb6c49",
   "metadata": {},
   "source": [
    "## input embedding\n",
    "> converts the subword tokenized input data into **embeddings** of size **(sequence,d_model)**\n",
    "* where **sequence** is total number of input tokens, and\n",
    "* **d_model** is the dimension of the transformer model that is fixed: **512**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27739eec-3615-4508-ba78-8a5ddd390045",
   "metadata": {},
   "source": [
    "**what happens to the input tokens after we feed them into the `Input Embedding Layer`?**\n",
    "> the **Input Embedding Layer** converts the input tokens into **512-dimensional embeddings** that is each token is represented by a vector of size 512, for example, say the size of the input tokens (after subword tokenization) is 6 then the output matrix will be of the size **(6,512)**.\n",
    "\n",
    "1. **input words ---> subword tokenizer ---> tokens: (6,)**\n",
    "> * `[\"token1 token2 token3 token4\"]` ---> **BPE** ---> `[\"token1\", \"token2\", \"token3\", \"token4\", \"token5\", \"token6\"]`\n",
    "\n",
    "2. **tokens:(6,) ---> Input Embedding Layer ---> embeddings: (6,512)**\n",
    "> * token1 --> `[0.0004, 0.34, ... 512th value]` a single token is represented by a vector of size 512\n",
    "> * token2 --> `[0.0004, 0.34, ... 512th value]` a single token is represented by a vector of size 512\n",
    "> * token3 --> `[0.0004, 0.34, ... 512th value]` a single token is represented by a vector of size 512\n",
    "> * token4 --> `[0.0004, 0.34, ... 512th value]` a single token is represented by a vector of size 512\n",
    "> * token5 --> `[0.0004, 0.34, ... 512th value]` a single token is represented by a vector of size 512\n",
    "> * token6 --> `[0.0004, 0.34, ... 512th value]` a single token is represented by a vector of size 512\n",
    "\n",
    "now, these embeddings are fed into the Econder Block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ecea3852-c871-4fbd-9d71-7c7fc61cb856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model:int, vocab_size:int) -> None:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            d_model (int): size of the word embedding vector\n",
    "            vocab_size (int): vocabulary size -> how many words are there in the vocabulary\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.vocab_size=vocab_size\n",
    "        self.embedding=nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=d_model\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.embedding(x) * torch.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1de697d-f623-41b7-b393-adf0179db064",
   "metadata": {},
   "source": [
    "## positonal encoding\n",
    "Positional encoding is a way of adding information about the position of each word in a sequence to the word embeddings. This is important because the transformer model does not use recurrence or convolution, and therefore does not have any inherent notion of word order. Positional encoding is done by mapping each position to a vector of the same size as the word embedding, and then adding them together. The vector for each position is computed using a combination of sine and cosine functions\n",
    "\n",
    "> * it captures the **position** of each word.\n",
    "> * computed only once and **reused** for every sentence during the **training** and **inference**.\n",
    "> * we add this **positional embedding matrix** with the **input embedding matrix**\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1272/1*YqVm4d_OmlE-J17r4i2yIg.png\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d19f69ac-4e00-4bc3-b843-d0f1dd6ce891",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model:int, seq_len:int, dropout:float) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model (int): size of the word embedding vector\n",
    "            seq_len (int): maximum length of the sentence\n",
    "            dropout (_type_): to reduce overfitting\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model-d_model\n",
    "        self.seq_len=seq_len\n",
    "        self.dropout=nn.Dropout(p=dropout)\n",
    "\n",
    "        # create positional encoding matrix of shape (seq_len,d_model)\n",
    "        pe=torch.zeros(size=(seq_len, d_model))\n",
    "        # create a vector of shape (seq_len, 1)\n",
    "        position=torch.arange(start=0,end=seq_len, dtype=torch.float).unsqueeze(dim=1)\n",
    "        denominator=torch.exp(torch.arange(start=0,end=d_model,step=2).float() * (torch.log(10000)/d_model))\n",
    "        # apply the sin to even position\n",
    "        pe[:, ::2]=torch.sin(input=position*denominator)\n",
    "        # apply the cos to odd position\n",
    "        pe[:, 1::2]=torch.cos(input=position*denominator)\n",
    "\n",
    "        # convert dim to batch processing: (seq_len, d_model) ---> (1, seq_len, d_model)\n",
    "        pe=pe.unsqueeze(dim=0)\n",
    "\n",
    "        # save the tensor to the module but not as a learned parameter\n",
    "        self.register_buffer(name=\"pe\", tensor=pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # note that, we don't want those postional encoding(pe) tensor to be learned as it i created only once\n",
    "        x=x + (self.pe[:, :x.shape[1], :]).requires_grad(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a82dff-91e4-45e0-bde9-cf07d3841e94",
   "metadata": {},
   "source": [
    "## layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68caf3b-d6bf-45b4-be1e-85b0884e4a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
