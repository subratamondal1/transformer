{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29a4025d-7f2f-4d25-a597-f43029d81618",
   "metadata": {},
   "source": [
    "# transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3b33b5-08e8-4fd8-a879-bb7e4d0d7fb2",
   "metadata": {},
   "source": [
    "> transformer models are primarily built from two types of blocks **Encoder Block** and **Decoder Block**.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1280/1*oh6wljc7WoW8G-0KNNBJww.jpeg\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3476f242-362a-49db-8d4c-3d0d76922d72",
   "metadata": {},
   "source": [
    "## input words or tokens\n",
    "> before feeding the input words into the **Input Embedding layer** of the **Encoder Block**, we need to tokenize it with **subword tokenization**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72901077-df85-48ef-8561-be598d6b503c",
   "metadata": {},
   "source": [
    "for transformer model we neither use **word tokenization** nor we use **character tokenization**, instead we use something in between called **subword tokenization**.\n",
    "\n",
    "**subword tokenization** says that most common or frequent words should be left as is, but rarer words should be broken down into meaningful subwords.\n",
    "> * keep common words but break down rarer words into meaningful subwords.\n",
    "\n",
    "**but why `subword tokenization` though?**\n",
    "> because it has better chance of **handling** OOV(out of vocabulary) words while **reducing** the vocabulary size and **maintaining** the performance.\n",
    "\n",
    "**how would you `perform` subword tokenization though?**\n",
    "\n",
    "there are multiple ways to do it, but the most popular is **BPE (Byte Pair Encoding)** which is **not specific** to nlp but a **simple data compression algorithm**.\n",
    "\n",
    "> other popular subword tokenization algorithms are **wordpiece** used by BERT and **sentencepiece** is an open source alternative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1316580-b6c6-43d5-87af-3477a6513f49",
   "metadata": {},
   "source": [
    "## input embedding layer\n",
    "> converts the subword tokenized input data into **embeddings** of size **(sequence,d_model)**\n",
    "* where **sequence** is total number of input tokens, and\n",
    "* **d_model** is the dimension of the transformer model that is fixed: **512**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27158814-2975-465c-8c6c-d5b313455f84",
   "metadata": {},
   "source": [
    "**what happens to the input tokens after we feed them into the `Input Embedding Layer`?**\n",
    "> the **Input Embedding Layer** converts the input tokens into **512-dimensional embeddings** that is each token is represented by a vector of size 512, for example, say the size of the input tokens (after subword tokenization) is 6 then the output matrix will be of the size **(6,512)**.\n",
    "\n",
    "1. **input words ---> subword tokenizer ---> tokens: (6,)**\n",
    "> * `[\"token1 token2 token3 token4\"]` ---> **BPE** ---> `[\"token1\", \"token2\", \"token3\", \"token4\", \"token5\", \"token6\"]`\n",
    "\n",
    "2. **tokens:(6,) ---> Input Embedding Layer ---> embeddings: (6,512)**\n",
    "> * token1 --> `[0.0004, 0.34, ... 512th value]` a single token is represented by a vector of size 512\n",
    "> * token2 --> `[0.0004, 0.34, ... 512th value]` a single token is represented by a vector of size 512\n",
    "> * token3 --> `[0.0004, 0.34, ... 512th value]` a single token is represented by a vector of size 512\n",
    "> * token4 --> `[0.0004, 0.34, ... 512th value]` a single token is represented by a vector of size 512\n",
    "> * token5 --> `[0.0004, 0.34, ... 512th value]` a single token is represented by a vector of size 512\n",
    "> * token6 --> `[0.0004, 0.34, ... 512th value]` a single token is represented by a vector of size 512\n",
    "\n",
    "now, these embeddings are fed into the Econder Block.\n",
    "\n",
    "3. **embeddings: (6,512) ---> Encoder Block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8bc72f-cd24-46ba-9ec6-2c6fcc28da41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba793f39-2ba5-416a-9c11-2be2e78c7ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525b21c3-f443-4079-b904-f694e665641d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fa3430-054d-42af-ab86-61f8c1975fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de359e6c-ecee-4bb3-b52c-3983fee1130b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
